{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0xOJAvAa31x+w6M6lv1Fb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"AKYtQbX5UL42","executionInfo":{"status":"ok","timestamp":1642583278941,"user_tz":-420,"elapsed":897,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"outputs":[],"source":["import os\n","import re\n","import sys\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","\n","import re, string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drivePath = \"/content/drive/MyDrive/toxic-comments-classification-master/\" ##path to model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJxsk-E2USs_","executionInfo":{"status":"ok","timestamp":1642583301501,"user_tz":-420,"elapsed":22571,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}},"outputId":"d437b997-a3f7-46a2-dbf3-8de9ad0eb21e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from keras.models import load_model\n","import pickle\n","\n","# returns a compiled model\n","# identical to the previous one\n","model = load_model(drivePath+'toxic_classification.h5')\n","# loading\n","with open(drivePath+'tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"],"metadata":{"id":"Kv8DqfHpUXEE","executionInfo":{"status":"ok","timestamp":1642583306140,"user_tz":-420,"elapsed":4646,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop = stopwords.words('english')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pQdsCKWzAg5h","executionInfo":{"status":"ok","timestamp":1642583306141,"user_tz":-420,"elapsed":21,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}},"outputId":"095d12c7-d39d-4166-8be7-ace673716e53"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["def preprocess(text):\n","    text = text.lower() #lowercase text\n","    text=text.strip()  #get rid of leading/trailing whitespace \n","    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n","    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n","    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n","    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n","    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n","    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n","    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n","    \n","    return text\n","def stopword(string):\n","    a= [i for i in string.split() if i not in stopwords.words('english')]\n","    return ' '.join(a)"],"metadata":{"id":"xz60d1mTCO5E","executionInfo":{"status":"ok","timestamp":1642583306142,"user_tz":-420,"elapsed":13,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","max_len = 150\n","def predictToxicContext(context):\n","  context = preprocess(context)\n","  context = stopword(context)\n","  print(context)\n","  tokenized = tokenizer.texts_to_sequences([context])\n","  padded_token = pad_sequences(tokenized, max_len)\n","\n","  return \"Probability of toxic context : \"+ str(model.predict(padded_token)[0][0]*100)+\"%\""],"metadata":{"id":"GhXi3qEJUeGL","executionInfo":{"status":"ok","timestamp":1642583306143,"user_tz":-420,"elapsed":12,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["test_sentence = \"get your dick hard\"\n","predictToxicContext(test_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"EDAFN8b1UfTE","executionInfo":{"status":"ok","timestamp":1642583516827,"user_tz":-420,"elapsed":423,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}},"outputId":"249d5f10-9d32-4cac-af7a-9c184bee3394"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["get dick hard\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Probability of toxic context : 99.99828338623047%'"]},"metadata":{},"execution_count":10}]}]}