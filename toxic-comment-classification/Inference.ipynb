{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx6IQGFe1KD3m5JETmVP+F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"AKYtQbX5UL42","executionInfo":{"status":"ok","timestamp":1642490882134,"user_tz":-420,"elapsed":1642,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"outputs":[],"source":["import os\n","import re\n","import sys\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","\n","import re, string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drivePath = \"/content/drive/MyDrive/toxic-comments-classification-master/\" ##path to model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJxsk-E2USs_","executionInfo":{"status":"ok","timestamp":1642490904368,"user_tz":-420,"elapsed":19227,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}},"outputId":"8613b245-b7ac-414e-a18c-b98dcf8e5fed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from keras.models import load_model\n","import pickle\n","\n","# returns a compiled model\n","# identical to the previous one\n","model = load_model(drivePath+'toxic_classification.h5')\n","# loading\n","with open(drivePath+'tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"],"metadata":{"id":"Kv8DqfHpUXEE","executionInfo":{"status":"ok","timestamp":1642490916616,"user_tz":-420,"elapsed":9235,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","max_len = 150\n","def predictToxicContext(context):\n","  tokenized = tokenizer.texts_to_sequences([context])\n","  padded_token = pad_sequences(tokenized, max_len)\n","\n","  return \"Probability of toxic context : \"+ str(model.predict(padded_token)[0][0]*100)+\"%\""],"metadata":{"id":"GhXi3qEJUeGL","executionInfo":{"status":"ok","timestamp":1642490918302,"user_tz":-420,"elapsed":350,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["test_sentence = \"I'm fine thankyou nigger\"\n","predictToxicContext(test_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EDAFN8b1UfTE","executionInfo":{"status":"ok","timestamp":1642490943694,"user_tz":-420,"elapsed":503,"user":{"displayName":"Thang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02441816833595542822"}},"outputId":"550a4be4-a9ee-44a5-a67b-e64720527c66"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Probability of toxic context : 99.87379908561707%'"]},"metadata":{},"execution_count":7}]}]}